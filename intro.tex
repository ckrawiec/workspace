\documentclass[12pt]{article}

\begin{document}

%intros
// The universe is our home. Everything we've ever known and discovered lie within it. It is only natural that mankind desires to unravel its mysteries and learn the how's and why's of our existence. With every passing decade, we have gotten closer to accurately writing the history of our world. The tools with which we look up and out become more precise thanks to new technologies and better understanding of the experiments we wish to conduct.

// With greater precision we are able to ask more difficult questions of the gathered data. Cosmology, while broad in its description of the universe today, is rooted in its small beginnings. % evolution in detail from COBE/WMAP/Planck
//Only seventeen(?) years ago it was discovered that our universe is expanding at an accelerated rate. Type Ia supernovae are ``standard candles,'' thus plotting their distances as a function of cosmic time places constraints on the growth of space itself.. % details of measurement
//This expansion is driven by parameters that describe the energy distribution on various scales (correct?) Thus by measuring this distribution - that of matter and radiation - at different epochs, we can fit theories of the primordial and fundamental (pick one?) laws to produce estimates of these parameters. Therefore, while our immediate universe tells us an important story, following it back to early times, and thus far distances, requires telescopes with great power and great precision. For the constraints on these parameters to be small enough to make trustworthy distinctions between the various prevailing theories, as has been done over the past century. % other galaxies, special/general relativity, age, expansion, inflation

// The Dark Energy Survey aims to study the properties of dark energy in three main ways: supernovae, cluster number counts, and weak lensing. % photometric paper

// Gravitational lensing is a good tool for mapping dark matter because nearly everything we see on the sky is affected by it to some degree. Not only can one measure the mass of large structures like clusters, using the strong distortion of galaxies behind them, one can also make statistical measurements of distortions on much smaller scales. It is these measurements that hold information about (?? matter-density power spectrum). This power spectrum is described by cosmological parameters including $\Omega_{M}$, $\sigma_{8}$, and $w$. % cosmic shear and 2-pt papers, g-g lensing


// Traditionally, shear measurements are the main output of weak lensing studies. Estimators typically rely on some measure of galaxy ellipticity. For an ensemble of sources, the average ellipticity in the absence of lensing should be zero. Under lensing, the expectation value instead indicates the level of shear observed (observed signal?). While an individual galaxy's shape does not tell us much about the lensing, stacking many galaxies around similar lenses at simlar radii can give fairly precise results. % current S/N, various experiments (KiDS, CFHTLens)
% history of shear measurement

// Shape distortion is only half of the story when it comes to lensing. Sources are also magnified and stretched. But where average ellipticity has zero expection, flux and size have intrinsic values that are harder to divine. This makes magnification measurements in general more noisy than those of shear and hence why they have lagged behind in the literature. In fact, magnification actually has some advantages over shear. For one, it is much harder to measure shapes at high redshift than flux or size(?), which in the age of ever-increasing reach, makes magnification an appealing probe of matter at earlier times (epochs?) The lensing signal is also boosted by the increased lens-source separation %(true? in all cases?)

// One can imagine a few different ways of going about a magnification measurement - change in flux, change in size, or a combination of this in a change in number of galaxies seen in a given patch of sky around a lens. % comparison of these techniques
%experiments

// A growing number of high-profile studies have decided on the angular correlation function between lenses and sources as their estimator. This function can be predicted from a chosen cosmology. Obviously, the number density of sources will be correlated with their angular separation from a lens. This is not always a positive correlation however, as the balance of brightened flux and smeared out surface area can cause number density to increase or decrease depending on source flux and the magnitude limit of the given observation. Thus analyses using the $w(\theta)$ estimator typically will weight their results using the slope of the source luminosity function in magnitude bins. %experiments - SDSS, CFHTLens (S/N, lenses, sources)
% how did they deal with contaminants

// Magnification analysis has been attempted so far in DES using Redmagic galaxies - galaxies found using a red-sequence finding algorithm - as both lenses and sources % results from Manuel's paper
// But I would like to take advantage of the higher signal at high-z while also benefitting from a (likely) cleaner separation of sources and lenses - important if we wish to exclude physical correlations from galaxies at the same redshift. 

%Education and Public Outreach
// Historically, progress in astrophysics and cosmology has been benchmarked by telescopic surveys that are broad in scope and produce flashy data products. Visual band images from Hubble and SDSS and all-sky maps produced by WMAP and Planck are easily digested by the public and smoothly link the sky we see with the deeper physics that sculpts it. But in the current information climate, social media and (accessible?) graphic design allow easier teaching of more nuanced (complex?) experiments, like LIGO, to even more members of the public.

// It is a responsibility of scientists to report their discoveries and advances to the rest of humanity. 

\end{document}
